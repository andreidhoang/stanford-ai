# GLM-4.6 Scaled-Down Training Configuration
# 64 × H100 80GB GPUs
# Experimental configuration for smaller organizations
# Achieves ~70-80% of full-scale performance

# ============================================================================
# SCALED MODEL CONFIGURATION
# ============================================================================

model:
  model_type: "glm"
  model_name: "GLM-4.6-15B"  # Scaled-down version

  # Reduced scale while maintaining architecture principles
  vocab_size: 151552
  max_position_embeddings: 32768  # Reduced context (32K)
  hidden_size: 3072  # Reduced from 5120

  # Fewer layers
  num_hidden_layers: 24  # Reduced from 92
  num_attention_heads: 24  # Reduced from 96
  num_key_value_heads: 4  # Reduced from 8 (still 6:1 ratio)
  head_dim: 128

  # Smaller FFN
  intermediate_size: 8192  # Dense layers
  hidden_act: "silu"

  # Fewer experts
  num_experts: 32  # Reduced from 160
  num_experts_per_tok: 4  # Reduced from 8
  num_shared_expert: 1
  moe_intermediate_size: 1024  # Reduced from 1536
  first_k_dense_replace: 2  # First 2 layers dense
  routed_scaling_factor: 2.5

  # Same positional encoding strategy
  rope_theta: 1000000.0
  partial_rotary_factor: 0.5

  # Same normalization
  rms_norm_eps: 1.0e-5
  qk_norm: true
  qk_norm_eps: 1.0e-5

  attention_bias: false
  attention_dropout: 0.0
  hidden_dropout: 0.0
  tie_word_embeddings: true

  # MTP disabled for smaller model
  mtp_enabled: false

  initializer_range: 0.02
  use_cache: true

# Expected parameters: ~15B total, ~3B active
expected_parameters:
  total: 15000000000  # 15 billion
  active: 3000000000  # 3 billion

# ============================================================================
# HARDWARE CONFIGURATION
# ============================================================================

hardware:
  gpu_type: "H100"
  gpu_memory: "80GB"
  total_gpus: 64
  gpus_per_node: 8
  total_nodes: 8

  intra_node: "NVLink 4.0"
  inter_node: "InfiniBand HDR 200Gb/s"

  shared_storage: "NFS"
  checkpoint_storage: "Local SSD"
  dataset_storage: "Cloud object storage"

# ============================================================================
# DISTRIBUTED TRAINING CONFIGURATION
# ============================================================================

distributed:
  # Simplified parallelism for smaller scale
  tensor_parallel_size: 4  # Within node
  pipeline_parallel_size: 4  # Across nodes
  expert_parallel_size: 4  # For 32 experts
  data_parallel_size: 1  # 64 / (4 × 4 × 4) = 1

  num_stages: 4
  pipeline_schedule: "1f1b"

  # ZeRO optimization
  zero_optimization:
    stage: 2  # Stage 2 sufficient for smaller model
    offload_optimizer: false  # Keep on GPU
    offload_param: false
    overlap_comm: true
    contiguous_gradients: true
    reduce_bucket_size: 5e8
    allgather_bucket_size: 5e8

  communication:
    fp32_reduce_scatter: false
    gradient_clipping: 1.0
    overlap_grad_reduce: true
    use_sequence_parallel: true

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================

training:
  # Reduced training budget
  total_tokens: 100000000000  # 100 billion (vs 23 trillion)

  # Single phase simplified training
  batch_size: 1024  # Global batch size (tokens in thousands)
  micro_batch_size: 8  # Per-GPU
  gradient_accumulation_steps: 8
  sequence_length: 4096

  # Training duration
  estimated_days: 30
  total_steps: 24414  # 100B / (4M tokens per step)

  # Checkpointing
  save_interval: 500
  eval_interval: 250
  log_interval: 10

  # Activation checkpointing
  activation_checkpointing: true
  checkpoint_num_layers: 1

# ============================================================================
# OPTIMIZER CONFIGURATION
# ============================================================================

optimizer:
  # Muon optimizer
  muon:
    enabled: true
    lr: 0.015  # Slightly lower for smaller model
    momentum: 0.95
    nesterov: true
    backend: "newtonschulz5"
    ns_steps: 5

  # AdamW
  adamw:
    enabled: true
    lr: 0.0003
    betas: [0.9, 0.95]
    eps: 1.0e-8
    weight_decay: 0.1

  # Learning rate schedule
  lr_scheduler:
    type: "cosine"
    warmup_steps: 1000
    warmup_ratio: 0.01
    min_lr_ratio: 0.1

  max_grad_norm: 1.0
  precision: "bf16"
  fp32_master_weights: true

# ============================================================================
# MoE-SPECIFIC CONFIGURATION
# ============================================================================

moe:
  load_balancing:
    method: "loss_free"
    bias_lr: 0.001
    bias_clamp: [-5.0, 5.0]
    update_frequency: 1
    ema_alpha: 0.01

  # Shorter expert warm-up
  expert_warmup:
    enabled: true
    total_tokens: 2000000000  # 2B tokens
    initial_experts: 1
    final_experts: 4
    schedule: "linear"

  expert_dropout: 0.0

  router:
    jitter_noise: 0.0
    use_sigmoid: true
    normalize_weights: true

# ============================================================================
# DATA CONFIGURATION
# ============================================================================

data:
  train_data_path: "/data/glm46_scaled/train"
  eval_data_path: "/data/glm46_scaled/eval"

  # Simplified data mixture
  data_mixture:
    web_crawl: 0.35
    books: 0.20
    code: 0.20
    academic: 0.15
    wikipedia: 0.10

  tokenizer_path: "/models/glm46_tokenizer"
  max_seq_length: 4096
  packing: true
  packing_efficiency: 0.95

  quality_filters:
    min_chars: 100
    max_chars: 1000000
    min_alpha_ratio: 0.6
    deduplication: "minhash"
    toxicity_threshold: 0.5

# ============================================================================
# MONITORING AND LOGGING
# ============================================================================

monitoring:
  wandb:
    enabled: true
    project: "glm46-scaled-pretraining"
    entity: "your-org"
    log_model: false

  tensorboard:
    enabled: true
    log_dir: "/logs/tensorboard"

  metrics:
    - loss
    - perplexity
    - learning_rate
    - gradient_norm
    - expert_utilization
    - expert_balance_cv
    - tokens_per_second
    - gpu_memory_usage

  alerts:
    loss_spike_threshold: 2.0
    gpu_memory_threshold: 0.95
    expert_imbalance_threshold: 0.25

# ============================================================================
# CHECKPOINT CONFIGURATION
# ============================================================================

checkpointing:
  save_dir: "/checkpoints/glm46_scaled"
  save_interval: 500
  keep_last_n: 5

  save_optimizer_states: true
  save_rng_states: true
  save_lr_scheduler: true

  async_save: true
  num_checkpoint_workers: 2

# ============================================================================
# FAULT TOLERANCE
# ============================================================================

fault_tolerance:
  auto_restart: true
  max_restarts: 3
  health_check_interval: 60
  detect_stragglers: true
  straggler_threshold: 1.5

# ============================================================================
# PERFORMANCE TARGETS
# ============================================================================

performance:
  # Adjusted targets for smaller scale
  target_tokens_per_second: 50000  # 50K tokens/s (64 GPUs)
  target_tflops_per_gpu: 600
  target_mfu: 0.50  # 50% MFU

  target_pipeline_efficiency: 0.85
  target_communication_efficiency: 0.80

  target_gpu_memory_usage: 35  # GB per GPU
  target_activation_memory: 35  # GB per GPU

# ============================================================================
# COST ESTIMATION
# ============================================================================

cost:
  gpu_cost_per_hour: 3.00  # USD (H100 more expensive per GPU)
  total_gpu_hours: 46080  # 30 days × 24 hours × 64 GPUs
  estimated_compute_cost: 138240  # USD

  data_preprocessing: 10000  # USD
  storage: 5000  # USD
  networking: 2000  # USD

  total_estimated_cost: 155000  # USD (~$155K)

# ============================================================================
# EXPECTED PERFORMANCE
# ============================================================================

expected_performance:
  # Compared to full GLM-4.6
  parameter_ratio: 0.042  # 15B / 355B
  training_tokens_ratio: 0.0043  # 100B / 23T
  expected_capability: "70-80% of full model"

  # Benchmark estimates
  estimated_mmlu: "75-80"  # vs 87.2 for full model
  estimated_gsm8k: "85-90"  # vs 94.8
  estimated_humaneval: "70-75"  # vs 74.4

  # Use cases
  suitable_for:
    - "Research experiments"
    - "Architecture validation"
    - "Ablation studies"
    - "Small-scale production"
    - "Fine-tuning base"

  not_suitable_for:
    - "State-of-the-art benchmarks"
    - "Large-scale production"
    - "Complex reasoning tasks"
