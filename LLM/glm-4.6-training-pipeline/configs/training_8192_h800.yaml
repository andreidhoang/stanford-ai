# GLM-4.6 Production Training Configuration
# 8,192 × H800 80GB GPUs
# Matches official GLM-4.6 training setup exactly

# Import model configuration
model_config: "configs/model_355b_32b_active.yaml"

# ============================================================================
# HARDWARE CONFIGURATION
# ============================================================================

hardware:
  # GPU specifications
  gpu_type: "H800"
  gpu_memory: "80GB"
  total_gpus: 8192
  gpus_per_node: 8
  total_nodes: 1024

  # Interconnect
  intra_node: "NVLink 4.0"  # 900 GB/s bidirectional
  inter_node: "InfiniBand HDR 200Gb/s"

  # Storage
  shared_storage: "Distributed parallel file system"
  checkpoint_storage: "NVMe SSD"
  dataset_storage: "High-throughput object storage"

# ============================================================================
# DISTRIBUTED TRAINING CONFIGURATION
# ============================================================================

distributed:
  # Three-dimensional parallelism: TP × PP × EP × DP

  # Tensor Parallelism (splits model width)
  tensor_parallel_size: 8  # Within node (NVLink)

  # Pipeline Parallelism (splits model depth)
  pipeline_parallel_size: 16  # Across nodes (InfiniBand)
  num_stages: 16
  pipeline_schedule: "interleaved_1f1b"  # 1 forward, 1 backward

  # Expert Parallelism (distributes experts)
  expert_parallel_size: 32  # All-to-all communication

  # Data Parallelism (remaining dimension)
  data_parallel_size: 4  # 8192 / (8 × 16 × 32) = 2

  # ZeRO optimization
  zero_optimization:
    stage: 3  # Partition optimizer states, gradients, and parameters
    offload_optimizer: true  # Offload to NVMe
    offload_param: false  # Keep params on GPU
    overlap_comm: true  # Overlap communication with computation
    contiguous_gradients: true
    reduce_bucket_size: 5e8
    allgather_bucket_size: 5e8

  # Communication optimization
  communication:
    fp32_reduce_scatter: false  # Use BF16
    gradient_clipping: 1.0
    overlap_grad_reduce: true
    use_sequence_parallel: true  # For long sequences

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================

training:
  # Training phases
  total_tokens: 23000000000000  # 23 trillion

  # Phase 1: General pre-training (15T tokens)
  phase_1:
    tokens: 15000000000000
    batch_size: 4096  # Global batch size in tokens (thousands)
    micro_batch_size: 4  # Per-GPU micro-batch
    gradient_accumulation_steps: 32
    sequence_length: 4096

  # Phase 2: Long-context pre-training (7T tokens)
  phase_2:
    tokens: 7000000000000
    batch_size: 4096
    micro_batch_size: 2  # Reduced for longer sequences
    gradient_accumulation_steps: 64
    sequence_length: 32768  # Increased context

  # Phase 3: Curriculum learning (1T tokens)
  phase_3:
    tokens: 1000000000000
    batch_size: 4096
    micro_batch_size: 2
    gradient_accumulation_steps: 64
    sequence_length: 32768

  # Training duration
  estimated_days: 82
  steps_per_phase:
    phase_1: 3662109  # 15T / (4M tokens per step)
    phase_2: 1708984  # 7T / (4M tokens per step)
    phase_3: 244141   # 1T / (4M tokens per step)

  # Checkpointing
  save_interval: 1000  # Save every 1000 steps
  eval_interval: 500   # Evaluate every 500 steps
  log_interval: 10     # Log every 10 steps

  # Activation checkpointing
  activation_checkpointing: true
  checkpoint_num_layers: 1  # Checkpoint every layer

# ============================================================================
# OPTIMIZER CONFIGURATION
# ============================================================================

optimizer:
  # Muon optimizer for weight matrices
  muon:
    enabled: true
    lr: 0.02
    momentum: 0.95
    nesterov: true
    backend: "newtonschulz5"  # 5 Newton-Schulz iterations
    ns_steps: 5

  # AdamW for biases, norms, embeddings
  adamw:
    enabled: true
    lr: 0.0003
    betas: [0.9, 0.95]
    eps: 1.0e-8
    weight_decay: 0.1

  # Learning rate schedule
  lr_scheduler:
    type: "cosine"
    warmup_steps: 2000
    warmup_ratio: 0.01
    min_lr_ratio: 0.1  # min_lr = 0.1 * peak_lr

  # Gradient clipping
  max_grad_norm: 1.0

  # Mixed precision
  precision: "bf16"  # BFloat16 for training
  fp32_master_weights: true

# ============================================================================
# MoE-SPECIFIC CONFIGURATION
# ============================================================================

moe:
  # Expert balancing
  load_balancing:
    method: "loss_free"  # Loss-free dynamic bias adjustment
    bias_lr: 0.001
    bias_clamp: [-5.0, 5.0]
    update_frequency: 1  # Update every step
    ema_alpha: 0.01  # Exponential moving average for usage tracking

  # Expert warm-up (first 20B tokens)
  expert_warmup:
    enabled: true
    total_tokens: 20000000000
    initial_experts: 2
    final_experts: 8
    schedule: "linear"  # Linear ramp from 2 to 8

  # Expert dropout (for regularization)
  expert_dropout: 0.0  # No dropout in official GLM-4.6

  # Router configuration
  router:
    jitter_noise: 0.0  # No jitter noise
    use_sigmoid: true  # Sigmoid instead of softmax
    normalize_weights: true

# ============================================================================
# DATA CONFIGURATION
# ============================================================================

data:
  # Dataset paths
  train_data_path: "/data/glm46/train"
  eval_data_path: "/data/glm46/eval"

  # Data mixture (Phase 1: General pre-training)
  phase_1_mixture:
    web_crawl: 0.45      # Common Crawl, Web data
    books: 0.15          # Books, long-form content
    code: 0.15           # GitHub, code repositories
    academic: 0.10       # ArXiv, academic papers
    wikipedia: 0.05      # Wikipedia
    multilingual: 0.10   # Non-English data

  # Phase 2: Long-context data
  phase_2_mixture:
    books: 0.40          # Long books
    papers: 0.30         # Academic papers
    code: 0.20           # Long code files
    documentation: 0.10  # Technical docs

  # Phase 3: High-quality curriculum
  phase_3_mixture:
    math: 0.30           # Mathematical reasoning
    code: 0.25           # Code generation
    reasoning: 0.25      # Complex reasoning
    multilingual: 0.20   # Multilingual tasks

  # Data preprocessing
  tokenizer_path: "/models/glm46_tokenizer"
  max_seq_length: 32768  # Maximum in Phase 2/3
  packing: true          # Pack multiple sequences
  packing_efficiency: 0.95

  # Quality filtering
  quality_filters:
    min_chars: 100
    max_chars: 1000000
    min_alpha_ratio: 0.6
    deduplication: "minhash"
    toxicity_threshold: 0.5

# ============================================================================
# MONITORING AND LOGGING
# ============================================================================

monitoring:
  # Weights & Biases
  wandb:
    enabled: true
    project: "glm46-pretraining"
    entity: "your-org"
    log_model: false  # Don't upload model to W&B

  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "/logs/tensorboard"

  # Metrics to track
  metrics:
    - loss
    - perplexity
    - learning_rate
    - gradient_norm
    - expert_utilization
    - expert_balance_cv
    - tokens_per_second
    - gpu_memory_usage
    - communication_overhead

  # Alerts
  alerts:
    loss_spike_threshold: 2.0  # Alert if loss > 2x recent average
    gpu_memory_threshold: 0.95  # Alert if memory > 95%
    expert_imbalance_threshold: 0.2  # Alert if CV > 0.2

# ============================================================================
# CHECKPOINT CONFIGURATION
# ============================================================================

checkpointing:
  save_dir: "/checkpoints/glm46"
  save_interval: 1000
  keep_last_n: 10  # Keep last 10 checkpoints

  # Checkpoint content
  save_optimizer_states: true
  save_rng_states: true
  save_lr_scheduler: true

  # Async checkpointing
  async_save: true
  num_checkpoint_workers: 4

# ============================================================================
# FAULT TOLERANCE
# ============================================================================

fault_tolerance:
  # Automatic restart on failure
  auto_restart: true
  max_restarts: 3

  # Health checks
  health_check_interval: 60  # seconds

  # Straggler detection
  detect_stragglers: true
  straggler_threshold: 1.5  # 1.5x slower than average

# ============================================================================
# PERFORMANCE TARGETS
# ============================================================================

performance:
  # Throughput targets
  target_tokens_per_second: 2800000  # 2.8M tokens/s
  target_tflops_per_gpu: 700
  target_mfu: 0.61  # Model FLOPs Utilization: 61%

  # Efficiency targets
  target_pipeline_efficiency: 0.88  # 88% (12% bubble)
  target_communication_efficiency: 0.75  # 75%

  # Memory targets
  target_gpu_memory_usage: 40  # GB per GPU
  target_activation_memory: 40  # GB per GPU

# ============================================================================
# COST ESTIMATION
# ============================================================================

cost:
  gpu_cost_per_hour: 2.50  # USD
  total_gpu_hours: 16515072  # 82 days × 24 hours × 8192 GPUs
  estimated_compute_cost: 41287680  # USD

  # Additional costs
  data_preprocessing: 500000  # USD
  storage: 100000  # USD per month
  networking: 50000  # USD per month

  total_estimated_cost: 44600000  # USD (~$44.6M)
