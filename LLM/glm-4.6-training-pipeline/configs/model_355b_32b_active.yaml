# GLM-4.6 Full-Scale Model Configuration
# 355B total parameters, 32B active per forward pass
# Matches official GLM-4.6 architecture exactly

model:
  # Model identification
  model_type: "glm"
  model_name: "GLM-4.6-355B"

  # Vocabulary and embeddings
  vocab_size: 151552
  max_position_embeddings: 202752  # 200K context window
  hidden_size: 5120

  # Transformer architecture
  num_hidden_layers: 92
  num_attention_heads: 96
  num_key_value_heads: 8  # Grouped-Query Attention (12:1 ratio)
  head_dim: 128  # hidden_size / num_attention_heads = 5120 / 40 = 128

  # Feed-forward network
  intermediate_size: 12288  # Dense layers (first 3 layers)
  hidden_act: "silu"  # SwiGLU activation

  # Mixture-of-Experts configuration
  num_experts: 160  # Routed experts
  num_experts_per_tok: 8  # Top-8 routing
  num_shared_expert: 1  # Always-active shared expert
  moe_intermediate_size: 1536  # Expert FFN intermediate size
  first_k_dense_replace: 3  # First 3 layers use dense FFN
  routed_scaling_factor: 2.5  # Scaling factor for routed expert outputs

  # Positional encoding
  rope_theta: 1000000.0  # RoPE base frequency (1M for long context)
  partial_rotary_factor: 0.5  # Partial RoPE: 50% rotation

  # Normalization
  rms_norm_eps: 1.0e-5
  qk_norm: true  # QK-Normalization for stability
  qk_norm_eps: 1.0e-5

  # Layer configuration
  attention_bias: false  # No bias in attention projections
  attention_dropout: 0.0
  hidden_dropout: 0.0

  # Weight tying
  tie_word_embeddings: true  # Tie input/output embeddings

  # Multi-Token Prediction (MTP)
  mtp_enabled: true
  mtp_num_auxiliary_heads: 4  # Number of auxiliary prediction heads
  mtp_offset: 1  # Token offset for auxiliary predictions

  # Initialization
  initializer_range: 0.02
  use_cache: true  # Enable KV caching for generation

# Expected parameter counts (for validation)
expected_parameters:
  total: 355000000000  # 355 billion
  active: 32000000000  # 32 billion

  # Breakdown by component
  embeddings: 775000000  # vocab_size * hidden_size
  attention_per_layer: 157000000  # Q/K/V/O projections
  dense_ffn_per_layer: 252000000  # First 3 layers
  moe_per_layer: 13000000000  # Layers 4-92 (160 experts + 1 shared)
  lm_head: 775000000  # Same as embeddings if tied

# Model capabilities
capabilities:
  max_input_tokens: 200000
  max_output_tokens: 128000
  supports_streaming: true
  supports_batch_inference: true
  supports_quantization: true  # FP8, AWQ, GGUF

# License and attribution
metadata:
  license: "MIT"
  source: "Recreated from GLM-4.6 technical documentation"
  paper: "https://z.ai/blog/glm-4.6"
  huggingface: "zai-org/GLM-4.6"
