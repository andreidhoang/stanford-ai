### Architecture Comparison

All three models—Kimi K2, GLM-4.6, and MiniMax M2—adopt a Mixture-of-Experts (MoE) Transformer architecture as their core foundation. From first principles, the Transformer architecture itself stems from the fundamental need to model sequential data efficiently through self-attention, which captures long-range dependencies without the sequential bottlenecks of RNNs. Self-attention operates on the ground truth that information in sequences (e.g., language) is relational and context-dependent; by computing pairwise similarities between tokens, it enables parallelizable computation, scaling to massive datasets where O(n^2) complexity is mitigated via optimizations like multi-head attention. MoE extends this by introducing sparsity: instead of activating all parameters for every input, only a subset of "experts" (specialized sub-networks) are routed to per token. This decision arises from the first-principle constraint of computational resources—FLOPs are finite, and dense models waste compute on irrelevant parameters. Scaling laws (e.g., Chinchilla/Kaplan) show that performance improves with parameter count, but inference latency and memory explode; MoE balances this by increasing total capacity while keeping active compute low, akin to biological neural sparsity where not all neurons fire for every stimulus.

- **Kimi K2**: 1.04 trillion total parameters, 32 billion active (sparsity ratio ~48). 384 experts, 8 active per token, 1 shared expert. Uses Multi-head Latent Attention (MLA, inspired by DeepSeek-V3), with 64 attention heads, hidden dim 7168, expert hidden dim 2048, and a single dense layer. Context: 4K during pre-training, extended to 128K via YaRN (Yet another RoPE extensioN, a positional encoding tweak). Why these choices? From first principles, higher sparsity (more experts) allows greater specialization—each expert can focus on niches like syntax or semantics—while experiments showed performance gains under fixed FLOPs (e.g., validation loss drops with sparsity up to 48). Reducing heads from 128 to 64 prioritizes inference efficiency: fewer heads mean less parallel compute overhead, crucial for long contexts where attention is quadratic. Single dense layer simplifies the feed-forward network, reducing parameter bloat without loss, grounded in the principle that over-parameterization in FFNs often redundantly models linear transformations.
- **GLM-4.6**: 355 billion total parameters, 32 billion active (similar sparsity to Kimi). Uses Grouped-Query Attention (GQA) with 96 heads. Context: 200K tokens. Builds on GLM-4.5's MoE with loss-free balance routing and sigmoid gates for smoother expert activation. Why? GQA groups keys/values across heads, reducing KV cache size (a first-principle memory bottleneck in inference), enabling longer contexts without exploding VRAM—essential for real-world tasks like codebase analysis where context exceeds 128K. The 96 heads strike a balance: more than Kimi's 64 for finer-grained attention patterns (e.g., capturing multi-lingual nuances), but not excessive to avoid dim explosion. Sigmoid gates ensure continuous routing, preventing discrete jumps that could destabilize gradients, rooted in optimization theory where smooth functions converge better.

- **MiniMax M2**: 230 billion total parameters, 10 billion active (higher sparsity, ~23 ratio). Employs full attention (no sparse/linear variants), optimized for agentic workflows (plan-act-verify). Why full attention? From first principles, attention must capture global dependencies exhaustively; sparse/linear approximations (e.g., Lightning Attention) trade this for speed but falter in multi-hop reasoning (e.g., chain-of-thought where distant tokens interact). Experiments with hybrids showed deficits at scale—e.g., performance degradation in long-context math/code—because evaluation benchmarks (MMLU, MATH) saturate, hiding weaknesses. Infrastructure maturity favors full attention: linear variants are memory-bound in training and lack optimizations like prefix caching. Lower active params prioritize cost-effectiveness, aligning with the principle that agentic tasks need fast iteration, not maximal capacity.

In summary, all leverage MoE for efficient scaling, but Kimi emphasizes sparsity experimentation, GLM context length via GQA, and MiniMax full attention reliability for agents. A table for key metrics:

| Component      | Kimi K2         | GLM-4.6        | MiniMax M2                         |
| -------------- | --------------- | -------------- | ---------------------------------- |
| Total Params   | 1.04T           | 355B           | 230B                               |
| Active Params  | 32B             | 32B            | 10B                                |
| Experts/Active | 384 / 8         | Not specified  | Not specified                      |
| Attention Type | MLA (64 heads)  | GQA (96 heads) | Full attention                     |
| Context Length | 128K (extended) | 200K           | Not specified (long-context focus) |

### Pre-Training Techniques Comparison

Pre-training fundamentally aims to learn a probabilistic distribution over sequences (next-token prediction), grounded in information theory: maximize likelihood to compress and generalize from data entropy. Techniques like optimizers stabilize this high-dimensional optimization, while schedules prevent overfitting. All models use autoregressive loss, but augment with stability tricks.

- **Kimi K2**: MuonClip optimizer (Muon + QK-Clip), WSD learning rate (2e-4 constant, cosine decay), weight decay 0.1, batch 67M tokens. QK-Clip caps attention logits at 100 to prevent spikes, per-head rescaling. Infrastructure: H800 GPUs, expert/pipeline parallelism, FP8 storage, selective recomputation. Why? Logit explosions stem from unstable gradients in large models (first-principle: unbounded softmax amplifies outliers); clipping enforces bounded optimization, ensuring convergence without divergence. Parallelism addresses data parallelism limits in MoE (experts can't shard easily). Rephrasing pipeline enhances token utility—rephrasing data once equals multi-epoch gains without memorization risks, based on the principle that diverse phrasings reduce redundancy entropy.

- **GLM-4.6**: Multi-stage: 15T broad tokens, then 7T code/reasoning fine-tune, plus RL via "slime engine" for alignment. Hybrid reasoning: fast mode for simple, thinking mode with multi-token prediction head for CoT. Why? Multi-stage reflects staged learning (first principles: broad knowledge first, then specialization, mimicking human education). RL aligns to preferences, addressing the reward hacking problem in pure supervised fine-tuning. Thinking mode internalizes CoT, reducing inference steps—grounded in reasoning as search, where planning tokens predict future paths efficiently.

- **MiniMax M2**: Experimented with hybrid attention but settled on full; pre-training emphasized long-context data for agentic robustness. No specific optimizer/schedule details. Why? Full attention ensures complete dependency modeling, crucial for agents where partial attention misses subtle cues (e.g., in plan verification). Scaling prioritizes data quality over volume, as compute slows but data length grows—first-principle: better to model full relations than approximate sparsely.

### Datasets Comparison

Datasets are curated to minimize noise (low-entropy junk) and maximize coverage (high diversity), from the principle that models learn from data distribution; quality trumps quantity post-Chinchilla.

- **Kimi K2**: 15.5T tokens across Web Text, Code, Mathematics, Knowledge. Curation: Rigorous validation per Kimi K1.5. Rephrasing: Knowledge chunks autoregressively rephrased for style/diversity, fidelity-checked semantically; Math to "learning-note" style, translations. Examples: Knowledge—original: "The Eiffel Tower is in Paris." Rephrased: "From a historical perspective, Paris hosts the iconic Eiffel Tower built in 1889." Math—original theorem proof; rephrased as notebook with steps. Why? Rephrasing boosts utility without repetition, reducing overfitting (principle: varied views of same info strengthen invariance).

- **GLM-4.6**: 15T broad-domain (multilingual, emphasis on code/reasoning/conversational), +7T code-specific. Bilingual (English/Chinese), includes multi-turn coding. Examples: Code—full-stack dev tasks like "Implement a REST API in Python"; Reasoning—agentic scenarios like "Plan a travel itinerary using tools." Curation: Focused on China-specific for cultural relevance. Why? Multilingual broadens distribution (principle: language is universal, but biases skew if monolingual); code emphasis builds procedural reasoning, as programming is pure logic.

- **MiniMax M2**: No public volume/details; implied updated, multimodal, long-context data for agents/coding. Examples: Inferred—agent workflows like "Research topic X, verify facts"; Code—full-stack edits. Why? Focus on agentic data addresses the principle that pre-training must align with downstream (e.g., plan-act-verify needs sequential verification data); long-context counters recency bias in short data.

### Training Data Templates Comparison

Templates format inputs for consistency, grounded in tokenization invariance.

- **Kimi K2**: Standard autoregressive on tokenized text; no special template for pre-training. Post-training tool template: <|im_begin|> tool_declare ... with JSON schemas, e.g., "functions.weather:1" with args. Why? Simple for generality; structured for tools to enable compositionality.

- **GLM-4.6**: Hybrid: Non-thinking for direct, thinking with multi-token head for planning. Example: Input text; output predicts next + plan tokens. Why? Mimics human deliberation, optimizing for complex queries without external prompting.

- **MiniMax M2**: Not specified; likely standard text for full attention. Agentic focus implies structured workflows in post-training. Why? Full attention thrives on raw sequences, preserving natural formats.
