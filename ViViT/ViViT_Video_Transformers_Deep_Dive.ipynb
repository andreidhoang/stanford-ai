{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "7tZyQpGCHx_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r7WqObjWuYd9"
      },
      "outputs": [],
      "source": [
        "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "PCjSPeVqa1kO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, sequence_length=20, img_size=224, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            video_paths (list): List of file paths to video files.\n",
        "            labels (list): List of integer labels corresponding to the videos.\n",
        "            sequence_length (int): The number of frames to extract per video.\n",
        "            img_size (int): The height and width to resize frames to.\n",
        "            transform (callable, optional): PyTorch transforms for augmentation.\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.sequence_length = sequence_length\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # 1. Load the video\n",
        "        frames = self._load_video(video_path)\n",
        "\n",
        "        # 2. Convert to Tensor\n",
        "        # Result shape: (Seq_Len, Height, Width, Channels) -> (Seq_Len, Channels, Height, Width)\n",
        "        # PyTorch expects Channels before Spatial dimensions\n",
        "        frames = torch.FloatTensor(frames).permute(0, 3, 1, 2)\n",
        "\n",
        "        # 3. Normalize to [0, 1] usually handled here or in transforms\n",
        "        frames = frames / 255.0\n",
        "\n",
        "        # 4. Apply transforms if any (e.g. Normalization to ImageNet mean/std)\n",
        "        if self.transform:\n",
        "            # Note: Transforms usually expect (C, H, W), so we might need to loop or use specific video transforms\n",
        "            pass\n",
        "\n",
        "        return frames, torch.tensor(label)\n",
        "\n",
        "    def _load_video(self, path):\n",
        "        \"\"\"\n",
        "        Reads a video and extracts a fixed number of frames using uniform sampling.\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        try:\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        except:\n",
        "            total_frames = 0\n",
        "\n",
        "        # If we can't read frame count, we fall back to reading all and sampling later (slower)\n",
        "        # Here we assume we can read it.\n",
        "\n",
        "        # Calculate the interval to pick frames uniformly\n",
        "        # e.g., if total=100, seq_len=20, interval=5. We pick frame 0, 5, 10...\n",
        "        interval = max(1, total_frames // self.sequence_length)\n",
        "\n",
        "        for i in range(self.sequence_length):\n",
        "            # Jump to the specific frame position\n",
        "            frame_id = i * interval\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Resize\n",
        "            frame = cv2.resize(frame, (self.img_size, self.img_size))\n",
        "\n",
        "            # BGR (OpenCV default) to RGB\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Padding: If video is too short (fewer than sequence_length frames), pad with zeros\n",
        "        frames = np.array(frames)\n",
        "        if len(frames) < self.sequence_length:\n",
        "            pad_shape = (self.sequence_length - len(frames), self.img_size, self.img_size, 3)\n",
        "            padding = np.zeros(pad_shape)\n",
        "            if len(frames) > 0:\n",
        "                frames = np.concatenate((frames, padding), axis=0)\n",
        "            else:\n",
        "                # Handle completely broken video file\n",
        "                frames = np.zeros((self.sequence_length, self.img_size, self.img_size, 3))\n",
        "\n",
        "        return frames"
      ],
      "metadata": {
        "id": "rWig8Od9egXf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        # 1. Load Pre-trained Model (DenseNet121)\n",
        "        # 'weights=\"DEFAULT\"' loads the best available ImageNet weights\n",
        "        densenet = models.densenet121(weights='DEFAULT')\n",
        "\n",
        "        # 2. Extract the \"Body\"\n",
        "        # In DenseNet, the feature extractor is stored in .features\n",
        "        # Output shape of this part is (Batch, 1024, 7, 7) for 224x224 input\n",
        "        self.feature_extractor = densenet.features\n",
        "\n",
        "        # 3. Freeze Weights (Efficiency)\n",
        "        # We don't want to retrain the CNN, just use it. This saves massive memory.\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # 4. Global Average Pooling\n",
        "        # Converts spatial map (7x7) into a single vector (1x1)\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: (Batch_Size, Sequence_Length, 3, 224, 224)\n",
        "\n",
        "        batch_size, seq_len, c, h, w = x.size()\n",
        "\n",
        "        # 5. The \"Time Distributed\" Trick\n",
        "        # CNNs expect (Batch, Channels, Height, Width). They don't understand \"Time\".\n",
        "        # We merge Batch and Time dimensions to treat every frame as an independent image.\n",
        "        x = x.view(batch_size * seq_len, c, h, w)\n",
        "        # New shape: (Batch_Size * Seq_Len, 3, 224, 224)\n",
        "\n",
        "        # 6. Pass through CNN\n",
        "        out = self.feature_extractor(x)\n",
        "        # Output shape: (Batch * Seq_Len, 1024, 7, 7)\n",
        "\n",
        "        # 7. Pool and Flatten\n",
        "        out = self.pooling(out)\n",
        "        # Shape: (Batch * Seq_Len, 1024, 1, 1)\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        # Shape: (Batch * Seq_Len, 1024)\n",
        "\n",
        "        # 8. Restore Time Dimension\n",
        "        # We separate the frames back into their original videos\n",
        "        out = out.view(batch_size, seq_len, -1)\n",
        "        # Final Shape: (Batch_Size, Seq_Len, 1024)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "a7lQ23oyhper"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand what the extractor function is doing from first principles, we must acknowledge the fundamental problem of computer vision: **The Semantic Gap.**\n",
        "\n",
        "To a computer, an image of a cricket match is not \"sport.\" It is just a massive grid of 150,528 separate numbers (pixels). If you move the camera 1 inch to the left, every single one of those 150,528 numbers changes completely. That is \"noise.\"\n",
        "\n",
        "The **Feature Extractor** is a machine designed to destroy that noise and preserve only the **meaning**.\n",
        "\n",
        "Here is the step-by-step breakdown of what happens to your data inside that function.\n",
        "\n",
        "### 1. The Input: \"The Noisy Grid\"\n",
        "* **Data:** One frame of your video.\n",
        "* **Shape:** $224 \\times 224 \\times 3$ (Height, Width, Color).\n",
        "* **First Principle Status:** **High Dimensional, Low Semantic Value.**\n",
        "    * It has huge detail (you can see every blade of grass).\n",
        "    * It has zero understanding (it doesn't know \"grass\" is a thing).\n",
        "\n",
        "### 2. The Process: \"The Hierarchical Filter\"\n",
        "We pass this grid through **DenseNet121**, which is a stack of **Convolutional Filters**. Think of these as a series of sieves or scanners.\n",
        "\n",
        "\n",
        "\n",
        "* **Layer 1 (The Edge Detectors):**\n",
        "    * **What it does:** Scans for simple contrast changes. Vertical lines, horizontal lines, color blobs.\n",
        "    * **Result:** It realizes \"there is a vertical edge here (the wicket)\" and \"there is a horizontal edge here (the bat).\"\n",
        "* **Middle Layers (The Texture Detectors):**\n",
        "    * **What it does:** Combines edges into shapes.\n",
        "    * **Result:** It sees \"circular patterns\" (a head), \"woven textures\" (clothing), or \"green repetitive noise\" (grass).\n",
        "* **Final Layers (The Part Detectors):**\n",
        "    * **What it does:** Combines shapes into object parts.\n",
        "    * **Result:** It identifies \"a human leg,\" \"a wooden plank,\" \"a sphere.\"\n",
        "\n",
        "**Crucial Point:** As the data goes deeper, the *spatial size* shrinks (from $224 \\times 224$ to $7 \\times 7$), but the *depth of understanding* grows (from 3 color channels to 1024 feature channels).\n",
        "\n",
        "### 3. The Bottleneck: \"Global Average Pooling\"\n",
        "This is the specific line in our code: `nn.AdaptiveAvgPool2d((1, 1))`. This is the most aggressive step in the entire pipeline.\n",
        "\n",
        "* **The Input:** A $7 \\times 7$ grid with 1024 channels. This still has *location* info (e.g., \"The bat is in the top-left corner\").\n",
        "* **The Action:** We take the **Average** of the entire $7 \\times 7$ grid.\n",
        "* **The Result:** A single $1 \\times 1$ vector.\n",
        "* **First Principle Logic:** **Invariance.**\n",
        "    * If the batsman moves slightly to the left, the $7 \\times 7$ grid changes.\n",
        "    * But the *average* of the grid remains roughly the same.\n",
        "    * We are trading **\"Where is it?\"** (Spatial location) for **\"Is it there?\"** (Semantic presence).\n",
        "\n",
        "### 4. The Output: \"The Semantic Summary\"\n",
        "* **Data:** A single vector of length 1024.\n",
        "* **First Principle Status:** **Low Dimensional, High Semantic Value.**\n",
        "\n",
        "Think of this 1024-length vector as a **Checklist of Concepts**.\n",
        "* Index 0 might measure \"Greenness/Grass-like\".\n",
        "* Index 1 might measure \"Metallic texture\".\n",
        "* Index 2 might measure \"Human face shape\".\n",
        "* Index 1023 might measure \"Sky blue\".\n",
        "\n",
        "**Example: A \"Cricket Shot\" Frame**\n",
        "* **Raw Pixels:** Millions of green and white numbers.\n",
        "* **Extracted Vector:**\n",
        "    * Index 0 (Grass): **0.9** (High)\n",
        "    * Index 50 (Wood texture): **0.8** (High)\n",
        "    * Index 200 (Water/Ocean): **0.01** (Low)\n",
        "    * Index 300 (Human shape): **0.95** (High)\n",
        "\n",
        "### Summary of Transformation\n",
        "\n",
        "| Stage | Data Representation | What the Computer \"Sees\" |\n",
        "| :--- | :--- | :--- |\n",
        "| **Input** | $224 \\times 224$ Pixels | \"Pixel (10,10) is Green.\" |\n",
        "| **Convolution** | $7 \\times 7$ Feature Maps | \"There is a vertical edge in the top left.\" |\n",
        "| **Output** | **1024 Vector** | **\"This image contains: Grass, Person, Bat.\"** |\n",
        "\n",
        "**Why do we need this for the Transformer?**\n",
        "The Transformer is a \"Sequence Learner.\" It's like a grammar checker. It doesn't want to look at pixels; it wants to look at *events*.\n",
        "By giving it these vectors, we are essentially feeding it a story:\n",
        "* **Frame 1:** [Person, Grass, Bat]\n",
        "* **Frame 2:** [Person, Grass, Bat]\n",
        "* **Frame 3:** [Person, Grass, Bat, **Motion Blur**]\n",
        "* **Frame 4:** [Person, Grass, Bat, **Impact**]\n",
        "\n",
        "The Transformer reads this sequence of concepts and concludes: **\"This is a Cricket Shot.\"**"
      ],
      "metadata": {
        "id": "c96N2zW5nEwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- COMPONENT 1: Positional Embedding ---\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, sequence_length, output_dim):\n",
        "        super().__init__()\n",
        "        # Keras: layers.Embedding\n",
        "        # PyTorch: nn.Embedding\n",
        "        self.position_embeddings = nn.Embedding(num_embeddings=sequence_length, embedding_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # We create a buffer for positions [0, 1, 2... seq_len]\n",
        "        # register_buffer ensures this tensor is saved with the model but not updated by gradients\n",
        "        self.register_buffer('position_ids', torch.arange(sequence_length))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs shape: (Batch, Seq_Len, Features)\n",
        "\n",
        "        # We slice the position_ids to match the current batch's sequence length\n",
        "        # (Just in case the input is shorter than max length)\n",
        "        length = inputs.size(1)\n",
        "        positions = self.position_ids[:length]\n",
        "\n",
        "        # Get embeddings: (Seq_Len, Features)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "\n",
        "        # Broadcast add: (Batch, Seq, Feat) + (Seq, Feat) works automatically in PyTorch\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "\n",
        "# --- COMPONENT 2: Transformer Encoder Block ---\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # 1. Attention\n",
        "        # batch_first=True makes it accept (Batch, Seq, Feature)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=0.3, batch_first=True)\n",
        "\n",
        "        # 2. Feed Forward Network (Dense Project)\n",
        "        # Keras: [Dense(dense_dim, gelu), Dense(embed_dim)]\n",
        "        self.dense_proj = nn.Sequential(\n",
        "            nn.Linear(embed_dim, dense_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dense_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        # 3. Layer Norms\n",
        "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
        "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: (Batch, Seq, Features)\n",
        "\n",
        "        # A. Attention Block\n",
        "        # PyTorch MultiheadAttention requires Q, K, V. For self-attention, all are 'inputs'.\n",
        "        # It returns (attn_output, attn_weights). We only need output.\n",
        "        attention_output, _ = self.attention(inputs, inputs, inputs)\n",
        "\n",
        "        # Add & Norm\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "\n",
        "        # B. Feed Forward Block\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "\n",
        "        # Add & Norm\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "# --- COMPONENT 3: The Full Model Assembly ---\n",
        "class VideoTransformerClassifier(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, dense_dim, num_heads, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Positional Embedding\n",
        "        self.pos_embedding = PositionalEmbedding(sequence_length, embed_dim)\n",
        "\n",
        "        # 2. Transformer Encoder\n",
        "        self.transformer_layer = TransformerEncoder(embed_dim, dense_dim, num_heads)\n",
        "\n",
        "        # 3. Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # 4. Classifier Head\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch, Seq, Embed_Dim)\n",
        "\n",
        "        x = self.pos_embedding(x)\n",
        "        x = self.transformer_layer(x)\n",
        "\n",
        "        # Global Max Pooling 1D\n",
        "        # Keras GlobalMaxPooling1D finds the max value across the Time dimension.\n",
        "        # PyTorch: x.max(dim=1) returns (values, indices). We take [0] for values.\n",
        "        x, _ = x.max(dim=1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Final Dense Layer\n",
        "        # Note: We do NOT use Softmax here. PyTorch CrossEntropyLoss does Softmax internally.\n",
        "        output = self.fc(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "FaTTPXBHoEOb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP: Load the pre-computed Data ---\n",
        "# Load tensors from Step 2.5\n",
        "train_features = torch.load(\"train_features.pt\")\n",
        "train_labels = torch.load(\"train_labels.pt\")\n",
        "\n",
        "# Create PyTorch Datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "# test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# --- HYPERPARAMETERS (Matching the Keras Code) ---\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "DENSE_DIM = 4\n",
        "NUM_HEADS = 1\n",
        "CLASSES = 5 # Depending on your dataset\n",
        "EPOCHS = 50 # Adjusted\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- INITIALIZE MODEL ---\n",
        "model = VideoTransformerClassifier(\n",
        "    sequence_length=MAX_SEQ_LENGTH,\n",
        "    embed_dim=NUM_FEATURES,\n",
        "    dense_dim=DENSE_DIM,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_classes=CLASSES\n",
        ").to(DEVICE)\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4) # Slightly lower LR often helps Transformers\n",
        "criterion = nn.CrossEntropyLoss() # This includes Softmax\n",
        "\n",
        "# --- THE TRAINING LOOP (The \"fit\" replacement) ---\n",
        "def run_experiment_pytorch():\n",
        "    best_acc = 0.0\n",
        "\n",
        "    print(f\"Starting training on {DEVICE}...\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train() # Set to training mode (enables Dropout)\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # 1. Zero Gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2. Forward Pass\n",
        "            outputs = model(features)\n",
        "\n",
        "            # 3. Calculate Loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 4. Backward Pass\n",
        "            loss.backward()\n",
        "\n",
        "            # 5. Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track stats\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # # --- VALIDATION LOOP ---\n",
        "        # model.eval() # Set to evaluation mode (disables Dropout)\n",
        "        # val_correct = 0\n",
        "        # val_total = 0\n",
        "        # with torch.no_grad():\n",
        "        #     for features, labels in test_loader:\n",
        "        #         features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
        "        #         outputs = model(features)\n",
        "        #         _, predicted = torch.max(outputs.data, 1)\n",
        "        #         val_total += labels.size(0)\n",
        "        #         val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        # print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "        #       f\"Loss: {running_loss/len(train_loader):.4f} | \"\n",
        "        #       f\"Train Acc: {train_acc:.2f}% | \"\n",
        "        #       f\"Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # # Save Best Model (Checkpoint)\n",
        "        # if val_acc > best_acc:\n",
        "        #     best_acc = val_acc\n",
        "        #     torch.save(model.state_dict(), \"best_video_transformer.pth\")\n",
        "        #     print(\"  -> Model Saved!\")\n",
        "\n",
        "# Run it!\n",
        "run_experiment_pytorch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XYlGcBYxuag",
        "outputId": "26c8a469-f150-46f5-935e-6e27ba81a9ce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cpu...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSyDFbiYx1C9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}