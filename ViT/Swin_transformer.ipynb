{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Swin Transformer\n",
        "\n",
        "- Functions:\n",
        "  1. window partition\n",
        "  2. window reverse\n",
        "- Classes:\n",
        "  1. Window Attention\n",
        "  2. Swin Block\n",
        "  3. Patch Marge\n",
        "  4. Multi-stage Swin Transformer\n",
        "- Datasets\n",
        "- Train + Test"
      ],
      "metadata": {
        "id": "nw_49_j3Uv_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Zfz-rrQoCA8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions:\n",
        "\n",
        "- window partition\n",
        "- window reverse\n"
      ],
      "metadata": {
        "id": "s72_NOkwCZoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, win):\n",
        "  \"\"\"\n",
        "  window partition function for Swin Transformer\n",
        "\n",
        "  input:\n",
        "  - x: (B, H, W, C)\n",
        "  - win: window size\n",
        "\n",
        "  output:\n",
        "  - x: (B * num_wins, win, win, C)\n",
        "\n",
        "  \"\"\"\n",
        "  B, H, W, C = x.shape\n",
        "  x = x.view(B, H//win, win, W//win, win, C)\n",
        "  x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
        "  x = x.view(-1, win, win, C)\n",
        "  # (win, win) == window size, (H//win, W//win) == total number of windows\n",
        "  return x\n",
        "\n",
        "def window_reverse(windows, win, W, H):\n",
        "  \"\"\"\n",
        "  window reverse function for Swin Transformer\n",
        "\n",
        "  input:\n",
        "  - windows: (B, H, W, C)\n",
        "  - win: window size\n",
        "  - W: width of image\n",
        "  - H: height of image\n",
        "\n",
        "  output:\n",
        "  - x: (B, H, W, C)\n",
        "  \"\"\"\n",
        "  B = int(windows.shape[0] / (H//win * W//win))\n",
        "  x = windows.view(B, H//win, W//win, win, win, -1)\n",
        "  x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
        "  x = x.view(B, H, W, -1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "FTfWY6_MCNM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Window Attention Module for Swin Transformer\n",
        "\n",
        "  input:\n",
        "  - x: (B, H, W, C)\n",
        "  - win: window size\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dim, num_heads, win):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = self.dim // self.num_heads\n",
        "    self.scale = self.head_dim ** -0.5\n",
        "    self.win = win\n",
        "\n",
        "    self.q = nn.Linear(self.dim, self.dim)\n",
        "    self.k = nn.Linear(self.dim, self.dim)\n",
        "    self.v = nn.Linear(self.dim, self.dim)\n",
        "\n",
        "    self.proj = nn.Linear(self.dim, self.dim)\n",
        "\n",
        "    # relative positional bias\n",
        "    coords = torch.stack(\n",
        "        torch.meshgrid(\n",
        "            torch.arange(self.win),\n",
        "            torch.arange(self.win),\n",
        "            indexing='ij'\n",
        "        )\n",
        "    )\n",
        "    coords_flat = coords.flatten(1)\n",
        "\n",
        "    rel = coords_flat[:, :, None] - coords_flat[:, None, :]\n",
        "\n",
        "    rel = rel.permute(1, 2, 0)\n",
        "    rel[:, :, 0] += (self.win - 1)\n",
        "    rel[:, :, 1] += (self.win - 1)\n",
        "\n",
        "    rel[:, :, 0] *= (2 * self.win - 1)\n",
        "    index = rel.sum(-1)\n",
        "\n",
        "    self.register_buffer('pos_index', index)\n",
        "    self.rel_bias = nn.Parameter(torch.zeros(2 * self.win - 1, 2 * self.win - 1, self.num_heads))\n",
        "    # mask value\n",
        "\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    B_, N, C = x.shape\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    q = q * self.scale\n",
        "    attn = q @ k.transpose(-2, -1)\n",
        "\n",
        "    rb = self.rel_bias[self.position_index.view(-1)].view(N, N, -1)\n",
        "    attn = attn + rb.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    if mask is not None:\n",
        "      nw = mask.shape[0]\n",
        "      attn = attn.view(B_//nw, nw, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "      attn = attn.view(-1, self.num_heads, N, N)\n",
        "\n",
        "    attn = F.softmax(attn, dim=-1)\n",
        "    out = attn @ v\n",
        "    out = out.transpose(1, 2).reshape(B_, N, C)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "aVCqlKkoEvJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## examples of rel"
      ],
      "metadata": {
        "id": "b7f3hRMjEY1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win=3\n",
        "coords = torch.stack(\n",
        "        torch.meshgrid(\n",
        "            torch.arange(3),\n",
        "            torch.arange(3),\n",
        "            indexing='ij'\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(\"coords: \", coords)\n",
        "coords_flat = coords.flatten(1)\n",
        "print(\"\\n\")\n",
        "print(\"coords_flat: \", coords_flat)\n",
        "print(\"\\n\")\n",
        "rel = coords_flat[:, :, None] - coords_flat[:, None, :]\n",
        "print(\"rel: \", rel)\n",
        "rel = rel.permute(1, 2, 0)\n",
        "rel[:, :, 0] += (win - 1)\n",
        "rel[:, :, 1] += (win - 1)\n",
        "rel[:, :, 0] *= (2 * win - 1)\n",
        "print(rel)\n",
        "index = rel.sum(-1)\n",
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_GwXRncHGty",
        "outputId": "77fb4cbf-9ade-48e8-c303-d310a0201d2c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coords:  tensor([[[0, 0, 0],\n",
            "         [1, 1, 1],\n",
            "         [2, 2, 2]],\n",
            "\n",
            "        [[0, 1, 2],\n",
            "         [0, 1, 2],\n",
            "         [0, 1, 2]]])\n",
            "\n",
            "\n",
            "coords_flat:  tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "        [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
            "\n",
            "\n",
            "rel:  tensor([[[ 0,  0,  0, -1, -1, -1, -2, -2, -2],\n",
            "         [ 0,  0,  0, -1, -1, -1, -2, -2, -2],\n",
            "         [ 0,  0,  0, -1, -1, -1, -2, -2, -2],\n",
            "         [ 1,  1,  1,  0,  0,  0, -1, -1, -1],\n",
            "         [ 1,  1,  1,  0,  0,  0, -1, -1, -1],\n",
            "         [ 1,  1,  1,  0,  0,  0, -1, -1, -1],\n",
            "         [ 2,  2,  2,  1,  1,  1,  0,  0,  0],\n",
            "         [ 2,  2,  2,  1,  1,  1,  0,  0,  0],\n",
            "         [ 2,  2,  2,  1,  1,  1,  0,  0,  0]],\n",
            "\n",
            "        [[ 0, -1, -2,  0, -1, -2,  0, -1, -2],\n",
            "         [ 1,  0, -1,  1,  0, -1,  1,  0, -1],\n",
            "         [ 2,  1,  0,  2,  1,  0,  2,  1,  0],\n",
            "         [ 0, -1, -2,  0, -1, -2,  0, -1, -2],\n",
            "         [ 1,  0, -1,  1,  0, -1,  1,  0, -1],\n",
            "         [ 2,  1,  0,  2,  1,  0,  2,  1,  0],\n",
            "         [ 0, -1, -2,  0, -1, -2,  0, -1, -2],\n",
            "         [ 1,  0, -1,  1,  0, -1,  1,  0, -1],\n",
            "         [ 2,  1,  0,  2,  1,  0,  2,  1,  0]]])\n",
            "tensor([[[10,  2],\n",
            "         [10,  1],\n",
            "         [10,  0],\n",
            "         [ 5,  2],\n",
            "         [ 5,  1],\n",
            "         [ 5,  0],\n",
            "         [ 0,  2],\n",
            "         [ 0,  1],\n",
            "         [ 0,  0]],\n",
            "\n",
            "        [[10,  3],\n",
            "         [10,  2],\n",
            "         [10,  1],\n",
            "         [ 5,  3],\n",
            "         [ 5,  2],\n",
            "         [ 5,  1],\n",
            "         [ 0,  3],\n",
            "         [ 0,  2],\n",
            "         [ 0,  1]],\n",
            "\n",
            "        [[10,  4],\n",
            "         [10,  3],\n",
            "         [10,  2],\n",
            "         [ 5,  4],\n",
            "         [ 5,  3],\n",
            "         [ 5,  2],\n",
            "         [ 0,  4],\n",
            "         [ 0,  3],\n",
            "         [ 0,  2]],\n",
            "\n",
            "        [[15,  2],\n",
            "         [15,  1],\n",
            "         [15,  0],\n",
            "         [10,  2],\n",
            "         [10,  1],\n",
            "         [10,  0],\n",
            "         [ 5,  2],\n",
            "         [ 5,  1],\n",
            "         [ 5,  0]],\n",
            "\n",
            "        [[15,  3],\n",
            "         [15,  2],\n",
            "         [15,  1],\n",
            "         [10,  3],\n",
            "         [10,  2],\n",
            "         [10,  1],\n",
            "         [ 5,  3],\n",
            "         [ 5,  2],\n",
            "         [ 5,  1]],\n",
            "\n",
            "        [[15,  4],\n",
            "         [15,  3],\n",
            "         [15,  2],\n",
            "         [10,  4],\n",
            "         [10,  3],\n",
            "         [10,  2],\n",
            "         [ 5,  4],\n",
            "         [ 5,  3],\n",
            "         [ 5,  2]],\n",
            "\n",
            "        [[20,  2],\n",
            "         [20,  1],\n",
            "         [20,  0],\n",
            "         [15,  2],\n",
            "         [15,  1],\n",
            "         [15,  0],\n",
            "         [10,  2],\n",
            "         [10,  1],\n",
            "         [10,  0]],\n",
            "\n",
            "        [[20,  3],\n",
            "         [20,  2],\n",
            "         [20,  1],\n",
            "         [15,  3],\n",
            "         [15,  2],\n",
            "         [15,  1],\n",
            "         [10,  3],\n",
            "         [10,  2],\n",
            "         [10,  1]],\n",
            "\n",
            "        [[20,  4],\n",
            "         [20,  3],\n",
            "         [20,  2],\n",
            "         [15,  4],\n",
            "         [15,  3],\n",
            "         [15,  2],\n",
            "         [10,  4],\n",
            "         [10,  3],\n",
            "         [10,  2]]])\n",
            "tensor([[12, 11, 10,  7,  6,  5,  2,  1,  0],\n",
            "        [13, 12, 11,  8,  7,  6,  3,  2,  1],\n",
            "        [14, 13, 12,  9,  8,  7,  4,  3,  2],\n",
            "        [17, 16, 15, 12, 11, 10,  7,  6,  5],\n",
            "        [18, 17, 16, 13, 12, 11,  8,  7,  6],\n",
            "        [19, 18, 17, 14, 13, 12,  9,  8,  7],\n",
            "        [22, 21, 20, 17, 16, 15, 12, 11, 10],\n",
            "        [23, 22, 21, 18, 17, 16, 13, 12, 11],\n",
            "        [24, 23, 22, 19, 18, 17, 14, 13, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinBlock(nn.Module):\n",
        "  def __init__(self, dim, res, win, shift, heads):\n",
        "\n",
        "    self.dim = dim\n",
        "    self.res = res\n",
        "    self.win = win\n",
        "    self.shift = shift\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.attn = WindowAttention(dim, res, win, heads)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(dim, dim * 4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(dim * 4, dim)\n",
        "    )\n",
        "\n",
        "    H, W = res\n",
        "    if shift > 0:\n",
        "      self.mask = self.create_mash(H, W, win, shift)\n",
        "\n",
        "    else:\n",
        "      self.mask = None\n",
        "\n",
        "    def create_mask(self, H, W, win, shift):\n",
        "\n",
        "      mask = torch.zeros((1, H, W, 1))\n",
        "      count = 0\n",
        "      for h in (slice(0, -win), slice(- win, -shift), slice(-shift, None)):\n",
        "        for w in (slice(0, -win), slice(- win, -shift), slice(-shift, None)):\n",
        "          mask[:, h, w, :] = count\n",
        "          count += 1\n",
        "\n",
        "      mask = window_partition(mask, win)\n",
        "      mask = mask.view(-1, win * win)\n",
        "      mask = mask.unsqueeze(1) - mask.unsqueeze(2)\n",
        "      mask = mask.masked_fill(mask!=0, -10000.0)\n",
        "      return mask\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, L, C = x.shape\n",
        "    H, W = self.res\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = x.view(B, H, W, C)\n",
        "\n",
        "    if self.shift > 0:\n",
        "      x = torch.roll(x, shifts=(-self.shift, -self.shift), dims=(1, 2))\n",
        "      x[:, -self.shift:, -self.shift:, :] = -1\n",
        "\n",
        "    win_x = window_partition(x, self.win).view(-1, self.win * self.win, C)\n",
        "\n",
        "    mask = self.mask.to(x.device) if self.mask is not None else None\n",
        "    win_x = self.attn(win_x, mask=mask)\n",
        "\n",
        "    x = window_reverse(win_x, self.win, H, W)\n",
        "\n",
        "    if self.shift > 0:\n",
        "      x = torch.roll(x, shifts=(self.shift, self.shift), dims=(1, 2))\n",
        "\n",
        "    x = x[:, :H, :W, :] + shortcut\n",
        "    shortcut = x\n",
        "\n",
        "    x = self.norm2(x)\n",
        "\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "fB0pkG1vHNb6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerge(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    self.dim = dim\n",
        "\n",
        "    self.reduction = nn.Linear(dim * 4, dim*2, bias=False)\n",
        "    self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "  def forward(self, x, H, W):\n",
        "    B, L, C = x.shape\n",
        "    x = x.view(B, H, W, C)\n",
        "    x0 = x[:, 0::2, 0::2, :]\n",
        "    x1 = x[:, 1::2, 0::2, :]\n",
        "    x2 = x[:, 0::2, 1::2, :]\n",
        "    x3 = x[:, 1::2, 1::2, :]\n",
        "\n",
        "    x0 = x0.view(B, -1, C)\n",
        "    x1 = x1.view(B, -1, C)\n",
        "    x2 = x2.view(B, -1, C)\n",
        "    x3 = x3.view(B, -1, C)\n",
        "\n",
        "    x = torch.cat([x0, x1, x2, x3], dim=-1)\n",
        "    x = x.view(B, -1, C * 4)\n",
        "    x = self.reduction(x)\n",
        "    x = self.norm(x)\n",
        "    return x, H//2, W//2"
      ],
      "metadata": {
        "id": "Wn3kck7fEft4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TwoStageSwinMNIST(nn.Module):\n",
        "    def __init__(self, embed_dim=48, heads=3, win=7, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Patch embedding: 28x28 -> 14x14 with C = embed_dim\n",
        "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=2, stride=2)\n",
        "        initial_res = (14, 14)  # just for reference / debugging\n",
        "\n",
        "        # ----- stage 1 -----\n",
        "        # operates at 14x14 with dim = embed_dim\n",
        "        self.stage1_blocks = nn.Sequential(\n",
        "            SwinBlock(embed_dim, heads=heads, win=win, shift=0),\n",
        "            SwinBlock(embed_dim, heads=heads, win=win, shift=3),\n",
        "        )\n",
        "\n",
        "        # Patch merging: 14x14 -> 7x7, dim: embed_dim -> 2 * embed_dim\n",
        "        self.patch_merge1 = PatchMerging(embed_dim)\n",
        "\n",
        "        # ----- stage 2 -----\n",
        "        # operates at 7x7 with dim = 2 * embed_dim\n",
        "        dim2 = 2 * embed_dim\n",
        "        self.stage2_blocks = nn.Sequential(\n",
        "            SwinBlock(dim2, heads=heads, win=win, shift=0),\n",
        "            SwinBlock(dim2, heads=heads, win=win, shift=3),\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(dim2)\n",
        "        self.head = nn.Linear(dim2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, 1, 28, 28)\n",
        "        \"\"\"\n",
        "        # --- patch embedding ---\n",
        "        x = self.patch_embed(x)         # (B, C, 14, 14), C = embed_dim\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # flatten to sequence: (B, H*W, C)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, 196, C)\n",
        "\n",
        "        # --- stage 1 ---\n",
        "        x = self.stage1_blocks(x)        # still (B, 196, C)\n",
        "\n",
        "        # --- patch merge 1: 14x14 -> 7x7, dim x2 ---\n",
        "        x = self.patch_merge1(x)         # (B, 49, 2C)\n",
        "\n",
        "        # --- stage 2 ---\n",
        "        x = self.stage2_blocks(x)        # (B, 49, 2C)\n",
        "\n",
        "        # --- global pooling + classifier ---\n",
        "        x = self.norm(x)                 # (B, 49, 2C)\n",
        "        x = x.mean(dim=1)                # (B, 2C)  global average over tokens\n",
        "        logits = self.head(x)            # (B, num_classes)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "TIAjLC7VEfMB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sf4nA-WKHmdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}