# GLM-4.6 vLLM Docker Image
# Production-ready container for deploying GLM-4.6 with vLLM

FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-dev \
    python3-pip \
    git \
    wget \
    curl \
    build-essential \
    ninja-build \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python3 -m pip install --upgrade pip

# Install PyTorch with CUDA support
RUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install vLLM
RUN pip3 install vllm

# Install additional dependencies
RUN pip3 install \
    transformers \
    accelerate \
    sentencepiece \
    protobuf \
    fastapi \
    uvicorn \
    prometheus-client

# Create app directory
WORKDIR /app

# Copy deployment script
COPY deploy_vllm.sh /app/
RUN chmod +x /app/deploy_vllm.sh

# Create directory for model cache
RUN mkdir -p /root/.cache/huggingface

# Expose API port
EXPOSE 8000

# Expose metrics port
EXPOSE 9090

# Set default environment variables
ENV MODEL_NAME=zai-org/GLM-4.6
ENV TENSOR_PARALLEL_SIZE=4
ENV GPU_MEMORY_UTILIZATION=0.95
ENV MAX_NUM_SEQS=256
ENV PORT=8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

# Entry point
CMD python3 -m vllm.entrypoints.openai.api_server \
    --model ${MODEL_NAME} \
    --host 0.0.0.0 \
    --port ${PORT} \
    --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} \
    --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION} \
    --max-num-seqs ${MAX_NUM_SEQS} \
    --trust-remote-code \
    --enable-prefix-caching
